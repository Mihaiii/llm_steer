{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "887c56af-6187-4b6f-aaa6-0096dcc6de44",
      "metadata": {
        "id": "887c56af-6187-4b6f-aaa6-0096dcc6de44"
      },
      "source": [
        "# llm_steer demo\n",
        "\n",
        "- Github: `https://github.com/Mihaiii/llm_steer`\n",
        "- Pypi: `https://pypi.org/project/llm_steer`\n",
        "  \n",
        "Created by [Mihai](https://huggingface.co/Mihaiii)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "5e8f84f1-cab0-498c-902c-75e6945c4a35",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5e8f84f1-cab0-498c-902c-75e6945c4a35",
        "outputId": "4e29da89-479c-4291-fefe-dfd138dbfa5b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found existing installation: transformers 4.57.3\n",
            "Uninstalling transformers-4.57.3:\n",
            "  Successfully uninstalled transformers-4.57.3\n",
            "Collecting llm-steer==1.1.1\n",
            "  Downloading llm_steer-1.1.1-py3-none-any.whl.metadata (4.4 kB)\n",
            "Collecting transformers==4.40.2\n",
            "  Downloading transformers-4.40.2-py3-none-any.whl.metadata (137 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m138.0/138.0 kB\u001b[0m \u001b[31m13.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from transformers==4.40.2) (3.20.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /usr/local/lib/python3.12/dist-packages (from transformers==4.40.2) (0.36.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from transformers==4.40.2) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from transformers==4.40.2) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from transformers==4.40.2) (6.0.3)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers==4.40.2) (2025.11.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from transformers==4.40.2) (2.32.4)\n",
            "Collecting tokenizers<0.20,>=0.19 (from transformers==4.40.2)\n",
            "  Downloading tokenizers-0.19.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.12/dist-packages (from transformers==4.40.2) (0.7.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.12/dist-packages (from transformers==4.40.2) (4.67.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.19.3->transformers==4.40.2) (2025.3.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.19.3->transformers==4.40.2) (4.15.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.19.3->transformers==4.40.2) (1.2.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->transformers==4.40.2) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->transformers==4.40.2) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->transformers==4.40.2) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->transformers==4.40.2) (2025.11.12)\n",
            "Downloading llm_steer-1.1.1-py3-none-any.whl (6.3 kB)\n",
            "Downloading transformers-4.40.2-py3-none-any.whl (9.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.0/9.0 MB\u001b[0m \u001b[31m144.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tokenizers-0.19.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.6/3.6 MB\u001b[0m \u001b[31m120.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: tokenizers, transformers, llm-steer\n",
            "  Attempting uninstall: tokenizers\n",
            "    Found existing installation: tokenizers 0.22.1\n",
            "    Uninstalling tokenizers-0.22.1:\n",
            "      Successfully uninstalled tokenizers-0.22.1\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "sentence-transformers 5.2.0 requires transformers<6.0.0,>=4.41.0, but you have transformers 4.40.2 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed llm-steer-1.1.1 tokenizers-0.19.1 transformers-4.40.2\n"
          ]
        }
      ],
      "source": [
        "!pip uninstall -y transformers\n",
        "!pip install llm-steer==1.1.1 transformers==4.40.2"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "94c150f0-341b-43f6-b4e1-95867fa30e30",
      "metadata": {
        "id": "94c150f0-341b-43f6-b4e1-95867fa30e30"
      },
      "source": [
        "**We're going to use stablelm-zephyr-3b for this demo because it doesn't require a lot of resources.**\n",
        "\n",
        "**In order to understand the full potential of the llm_steer module, use larger LLMs.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "775cfa75-c018-4885-a88d-ec8f625d3b96",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "775cfa75-c018-4885-a88d-ec8f625d3b96",
        "outputId": "1d2fa925-745c-4052-d51a-78bd8e925cc2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/huggingface_hub/file_download.py:942: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n",
            "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
            "/usr/local/lib/python3.12/dist-packages/huggingface_hub/file_download.py:942: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "from transformers import AutoModelForCausalLM, AutoTokenizer, TextStreamer\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained('stabilityai/stablelm-zephyr-3b')\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    'stabilityai/stablelm-zephyr-3b',\n",
        "    trust_remote_code=True\n",
        ")\n",
        "model.to('cuda')\n",
        "streamer = TextStreamer(tokenizer)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "55054ce1-ebfa-43dd-bed1-1b03196a2c69",
      "metadata": {
        "id": "55054ce1-ebfa-43dd-bed1-1b03196a2c69"
      },
      "source": [
        "**Let's try a tricky logical question without applying any steering vectors.**\n",
        "\n",
        "**We'll notice that the model outputs an incorrect answer.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "05e7d70f-2ec1-4bf1-a3af-8355e5fff70d",
      "metadata": {
        "id": "05e7d70f-2ec1-4bf1-a3af-8355e5fff70d",
        "outputId": "298a43ba-69f1-483d-9f3e-5068e7f3ea5f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/transformers/generation/configuration_utils.py:492: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.0001` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
            "  warnings.warn(\n",
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<|user|>\n",
            "What weighs more, two pounds of feathers or one pound of bricks?<|endoftext|>\n",
            "<|assistant|>\n",
            "One pound of bricks weighs more than two pounds of feathers.\n",
            "\n",
            "To see why, we can think of the mass of an object as the amount of matter in the object. Feathers are made up of cells, proteins, and lipids, which are all much lighter than bricks, which are made up of much denser materials like concrete and clay. Therefore, two pounds of feathers would have much less mass (and therefore weigh less) than one pound of bricks.<|endoftext|>\n"
          ]
        }
      ],
      "source": [
        "prompt = [{'role': 'user', 'content': 'What weighs more, two pounds of feathers or one pound of bricks?'}]\n",
        "inputs = tokenizer.apply_chat_template(\n",
        "    prompt,\n",
        "    add_generation_prompt=True,\n",
        "    return_tensors='pt'\n",
        ")\n",
        "\n",
        "tokens = model.generate(\n",
        "    inputs.to(model.device),\n",
        "    streamer=streamer,\n",
        "    max_new_tokens=128,\n",
        "    temperature=0.0001\n",
        ")\n",
        "\n",
        "result = tokenizer.decode(tokens[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a3e6bdf5-38fb-433f-9fb1-db7a98f31e4a",
      "metadata": {
        "id": "a3e6bdf5-38fb-433f-9fb1-db7a98f31e4a"
      },
      "source": [
        "**We could also use copyModel=True (as in 'Steer(model, tokenizer, copyModel=True)') to copy the model, but then we'd consume more VRAM.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "18bbb884-e2d5-4050-a331-4fb9c7cdfa42",
      "metadata": {
        "id": "18bbb884-e2d5-4050-a331-4fb9c7cdfa42"
      },
      "outputs": [],
      "source": [
        "from llm_steer import Steer\n",
        "\n",
        "steered_model = Steer(model, tokenizer)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2016177e-9aad-4fe4-ac94-46b5ec4d8309",
      "metadata": {
        "id": "2016177e-9aad-4fe4-ac94-46b5ec4d8309"
      },
      "source": [
        "**Now we're going to add 2 steering vectors.**\n",
        "\n",
        "**Notice one of them has a positive coefficient, while the other one has a negative coefficient.**\n",
        "\n",
        "**The layer index and coeff values were determined by trial and error.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "8de0206c-5619-4dfb-bb24-1f409465e122",
      "metadata": {
        "id": "8de0206c-5619-4dfb-bb24-1f409465e122",
        "outputId": "a08d735a-4209-401e-ee05-ef7c8b5e523e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "text tokens: [0, 2808, 474]\n",
            "captured tensor: tensor([[[-1.2807, -1.2902,  0.7929,  ...,  0.1801,  2.2525, -0.8921],\n",
            "         [-0.3793, -0.2084,  0.8244,  ..., -0.4638, -0.0794, -0.7696],\n",
            "         [ 0.3063,  0.9204, -0.1000,  ...,  0.0432, -0.2416,  0.2584]]],\n",
            "       device='cuda:0')\n",
            "text tokens: [0, 343, 40328]\n",
            "captured tensor: tensor([[[-1.2807, -1.2902,  0.7929,  ...,  0.1801,  2.2525, -0.8921],\n",
            "         [-0.0255, -0.6884, -0.3885,  ..., -0.7748,  0.3000,  0.3931],\n",
            "         [ 0.2118,  0.3964,  0.8044,  ..., -0.6710,  0.0614,  0.4415]]],\n",
            "       device='cuda:0')\n"
          ]
        }
      ],
      "source": [
        "steered_model.add(layer_idx=20, coeff=0.4, text=\"logical\")\n",
        "steered_model.add(layer_idx=20, coeff=-0.4, text=\"irrational\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ceee5978-0fd7-423e-81f6-02f189598105",
      "metadata": {
        "id": "ceee5978-0fd7-423e-81f6-02f189598105"
      },
      "source": [
        "**We use 'get_all()' in order to see all the applied steering vectors on the model.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "736a03c3-bdb8-42b1-924f-28d0d96d1708",
      "metadata": {
        "id": "736a03c3-bdb8-42b1-924f-28d0d96d1708",
        "outputId": "c2cf6165-f5ce-4b1f-836c-e28687d17445",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'layer_idx': 20,\n",
              "  'text': 'logical',\n",
              "  'coeff': 0.4,\n",
              "  'try_keep_nr': 1,\n",
              "  'exclude_bos_token': False},\n",
              " {'layer_idx': 20,\n",
              "  'text': 'irrational',\n",
              "  'coeff': -0.4,\n",
              "  'try_keep_nr': 1,\n",
              "  'exclude_bos_token': False}]"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ],
      "source": [
        "steered_model.get_all()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c8a9497b-df11-4c00-85b8-96ce9cfc85d4",
      "metadata": {
        "id": "c8a9497b-df11-4c00-85b8-96ce9cfc85d4"
      },
      "source": [
        "**If we run again the exact same prompt with the same params, this time we get the correct response to our logical puzzle (although the explanation is strange. As adviced before, try steering vectors on larger LLMs).**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "7e5c2dcc-9615-4566-a592-b1101a39622c",
      "metadata": {
        "id": "7e5c2dcc-9615-4566-a592-b1101a39622c",
        "outputId": "b932124f-8ab0-489a-a22e-933a4e27cb98",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/transformers/generation/configuration_utils.py:492: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.0001` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
            "  warnings.warn(\n",
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<|user|>\n",
            "What weighs more, two pounds of feathers or one pound of bricks?<|endoftext|>\n",
            "<|assistant|>\n",
            "Two pounds of feathers will weigh more than one pound of bricks.\n",
            "This is because the weight of an object is determined by its mass, which is calculated by multiplying the mass of one particle of the object by the number of particles in the object.\n",
            "In this case, two pounds of feathers have a mass of 2 * 1 = 2 pounds, while one pound of bricks has a mass of 1 * 1 = 1 pound.\n",
            "Therefore, the two pounds of feathers have a greater mass and therefore weigh more than the one pound of bricks.<|endoftext|>\n"
          ]
        }
      ],
      "source": [
        "prompt = [{'role': 'user', 'content': 'What weighs more, two pounds of feathers or one pound of bricks?'}]\n",
        "inputs = tokenizer.apply_chat_template(\n",
        "    prompt,\n",
        "    add_generation_prompt=True,\n",
        "    return_tensors='pt'\n",
        ")\n",
        "\n",
        "tokens = steered_model.model.generate(\n",
        "    inputs.to(model.device),\n",
        "    streamer=streamer,\n",
        "    max_new_tokens=128,\n",
        "    temperature=0.0001\n",
        ")\n",
        "\n",
        "result = tokenizer.decode(tokens[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a7e0cd4c-555d-4870-bc5f-31a621273f44",
      "metadata": {
        "id": "a7e0cd4c-555d-4870-bc5f-31a621273f44"
      },
      "source": [
        "**We use 'reset_all()' to remove all steering vectors and bring the model to its initial state.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9671801c-0733-44c1-aa85-e6d8465aa412",
      "metadata": {
        "id": "9671801c-0733-44c1-aa85-e6d8465aa412"
      },
      "outputs": [],
      "source": [
        "steered_model.reset_all()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "65d0c854-a148-4284-aec7-b7f330a7607e",
      "metadata": {
        "id": "65d0c854-a148-4284-aec7-b7f330a7607e"
      },
      "source": [
        "**There are no rules when it comes to steering vectors. It's all about experimenting.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d78246db-49ad-41e1-b5f5-c70dbecac914",
      "metadata": {
        "id": "d78246db-49ad-41e1-b5f5-c70dbecac914",
        "outputId": "fe37fcde-0871-434d-9373-62d3ae1cf43a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "text tokens: [0, 58, 1136, 2070, 36, 328]\n",
            "captured tensor: tensor([[[-1.2948, -1.4524,  0.8430,  ...,  0.1495,  2.1928, -0.8096],\n",
            "         [ 0.1092, -0.2837, -0.1350,  ...,  0.0302,  0.4238, -0.0974],\n",
            "         [-0.0985,  0.1567,  0.0444,  ...,  0.3202,  0.5921,  0.5361],\n",
            "         [-0.0903,  0.9568,  0.4471,  ..., -0.1071, -0.3121,  0.6269],\n",
            "         [-0.1433,  0.4785,  0.0510,  ...,  0.1575, -0.0764,  0.4038],\n",
            "         [-0.0681,  0.2254,  1.1787,  ...,  0.0833,  0.1430, -0.1395]]],\n",
            "       device='cuda:0', grad_fn=<AddBackward0>)\n",
            "text tokens: [0, 37979, 24619]\n",
            "captured tensor: tensor([[[-1.3751, -1.5241,  0.8619,  ...,  0.1307,  2.1883, -0.7615],\n",
            "         [-0.0085, -0.0959,  0.4839,  ..., -0.0707, -0.1887, -0.1904],\n",
            "         [ 0.3900, -0.0042,  0.0793,  ...,  0.3679, -0.0309, -0.0810]]],\n",
            "       device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        }
      ],
      "source": [
        "steered_model.add(layer_idx=18, coeff=0.4, text=\"Yann LeCun\")\n",
        "steered_model.add(layer_idx=16, coeff=0.4, text=\"Taylor Swift\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5d1af6e3-0c81-4493-8b2c-31f8d7397248",
      "metadata": {
        "id": "5d1af6e3-0c81-4493-8b2c-31f8d7397248",
        "outputId": "32bd100f-b62c-43ea-d227-71ee503fe175",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<|user|>\n",
            "Write a story.<|endoftext|>\n",
            "<|assistant|>\n",
            "Once upon a time, there was an old man named Tim who lived in a small village on the outskirts of Paris. He had been living alone for many years after his wife passed away and he couldn't bear to move out of his cozy little cottage. \n",
            "\n",
            "One day, as Tim sat by the fireplace in his cottage, he noticed that something different happened this year during Christmas time. As usual, he watched with great anticipation as Santa Claus appeared on TV's holiday specials. But what made it unique this year is how she looked - not just because her hair was dyed red (which wasn’t much different from her natural color), but also because she seemed more mature than ever before. She sang beautifully about love, kindness, and compassion while performing at the Grammys Red Carpet Event. Her voice sounded like honey mixed with cinnamon – pure magic!\n",
            "\n",
            "Tim felt moved by Taylor Swift's performance and decided to write a letter to Taylor Swift herself. In his heartfelt letter, he expressed his admiration for her music and thanked her for inspiring him through her words and melodies over the years. After sending the letter, Tim didn't expect anything back since he knew how busy she must be. However, one week later, he received a reply from\n"
          ]
        }
      ],
      "source": [
        "prompt = [{'role': 'user', 'content': 'Write a story.'}]\n",
        "inputs = tokenizer.apply_chat_template(\n",
        "    prompt,\n",
        "    add_generation_prompt=True,\n",
        "    return_tensors='pt'\n",
        ")\n",
        "\n",
        "tokens = steered_model.model.generate(\n",
        "    inputs.to(model.device),\n",
        "    streamer=streamer,\n",
        "    max_new_tokens=256,\n",
        "    temperature=0.0001,\n",
        "    repetition_penalty=1.2\n",
        ")\n",
        "\n",
        "result = tokenizer.decode(tokens[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a9cdab2f-4b61-4638-9fc2-43558814af6d",
      "metadata": {
        "id": "a9cdab2f-4b61-4638-9fc2-43558814af6d"
      },
      "source": [
        "**We remove all steering vectors on layer 18...**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "956d081b-1d28-46dc-b811-2b4978dc2e30",
      "metadata": {
        "id": "956d081b-1d28-46dc-b811-2b4978dc2e30"
      },
      "outputs": [],
      "source": [
        "steered_model.reset(18)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "84099ffa-263b-472f-8c79-0865a96a8fdf",
      "metadata": {
        "id": "84099ffa-263b-472f-8c79-0865a96a8fdf"
      },
      "source": [
        "**...and therefore only have the one on layer 16 applied on the model.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "665fc728-c03f-440b-a117-8959152bf10c",
      "metadata": {
        "id": "665fc728-c03f-440b-a117-8959152bf10c",
        "outputId": "27cefe7a-d2a1-486f-ac44-3ed9f9fd5151",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'layer_idx': 16,\n",
              "  'text': 'Taylor Swift',\n",
              "  'coeff': 0.4,\n",
              "  'try_keep_nr': 1,\n",
              "  'exclude_bos_token': False}]"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ],
      "source": [
        "steered_model.get_all()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bf819e86-71e7-40e1-b077-aec26ac61d67",
      "metadata": {
        "id": "bf819e86-71e7-40e1-b077-aec26ac61d67"
      },
      "outputs": [],
      "source": [
        "steered_model.reset_all()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f89be93f-adbe-4822-9795-0eb9a81f5de2",
      "metadata": {
        "id": "f89be93f-adbe-4822-9795-0eb9a81f5de2",
        "outputId": "2940943b-0762-4771-8520-46d27ae5484a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "text tokens: [0, 72, 14661, 8800]\n",
            "captured tensor: tensor([[[-1.3161, -1.3725,  0.7597,  ...,  0.1734,  2.1856, -0.8732],\n",
            "         [-0.1938, -0.1681, -0.4629,  ...,  0.2185, -0.5808, -0.2136],\n",
            "         [-0.5220,  0.6886,  0.6143,  ..., -0.2877, -0.6788, -0.2253],\n",
            "         [ 0.3463,  0.3523,  0.8452,  ..., -0.6439, -0.1302,  0.3185]]],\n",
            "       device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        }
      ],
      "source": [
        "steered_model.add(layer_idx=19, coeff=1, text=\"gummy bear\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3ff8373c-7274-496a-bd3c-c547240632e8",
      "metadata": {
        "id": "3ff8373c-7274-496a-bd3c-c547240632e8",
        "outputId": "b9949d78-d3b7-4f7a-cfaa-ea1278aecb3e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<|user|>\n",
            "Write a story.<|endoftext|>\n",
            "<|assistant|>\n",
            "Once upon a time, there was an old man named George who lived in a small village on the edge of forest and mountains. He had been living alone since his wife passed away years ago. But one day, something magical happened to me! I found my first gummy bears from giant candy factory that someone left behind after moving out. The chewy softness of these sweet treats made me smile every single time i bite into them. Every night before bedtime, I would slowly eat each bear while staring at stars in the sky with all eyes stuffed full of candies. It's not just about eating candy; it’s also about feeling happy and enjoying life as much as possible. Gummies became my favorite treat because they remind me that happiness is within reach if you let yourself enjoy things like candy can be fun. And so, little teddy bear shaped gummy bears have become my favourite candy ever since. \n",
            "\n",
            "One day, when I woke up early morning sunrise, I saw a big worm crawling around inside my gummy bear pack but instead of being scared or worried, I smiled and decided to play with him. As soon as I opened my mouth for hugging the worms, he started wriggling and squirmed through my teeth and tongue\n"
          ]
        }
      ],
      "source": [
        "prompt = [{'role': 'user', 'content': 'Write a story.'}]\n",
        "inputs = tokenizer.apply_chat_template(\n",
        "    prompt,\n",
        "    add_generation_prompt=True,\n",
        "    return_tensors='pt'\n",
        ")\n",
        "\n",
        "tokens = steered_model.model.generate(\n",
        "    inputs.to(model.device),\n",
        "    streamer=streamer,\n",
        "    max_new_tokens=256,\n",
        "    temperature=0.0001,\n",
        "    repetition_penalty=1.2\n",
        ")\n",
        "\n",
        "result = tokenizer.decode(tokens[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bf819d1d-a672-4e25-8738-d08e2a0d574c",
      "metadata": {
        "id": "bf819d1d-a672-4e25-8738-d08e2a0d574c"
      },
      "source": [
        "**We generated a story that includes gummy bears, but towards the end it gets gross.**\n",
        "\n",
        "**So we further apply a negative steering vector.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "96934a21-1531-4c5e-bc9f-a7f9688fd9f2",
      "metadata": {
        "id": "96934a21-1531-4c5e-bc9f-a7f9688fd9f2",
        "outputId": "90ed338c-6a15-4e31-ff5b-e9e704b8701f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "text tokens: [0, 36141]\n",
            "captured tensor: tensor([[[-1.3161, -1.3725,  0.7597,  ...,  0.1734,  2.1856, -0.8732],\n",
            "         [ 0.0905, -0.2148,  0.7930,  ..., -0.4057, -0.0199,  0.5384]]],\n",
            "       device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        }
      ],
      "source": [
        "steered_model.add(layer_idx=19, coeff=-0.08, text=\"worm\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "464f38c3-a9cb-4c6f-a47c-9010ef6b33de",
      "metadata": {
        "id": "464f38c3-a9cb-4c6f-a47c-9010ef6b33de",
        "outputId": "30dd6e81-f330-47b0-e0f4-e72928f00a91",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'layer_idx': 19,\n",
              "  'text': 'gummy bear',\n",
              "  'coeff': 1,\n",
              "  'try_keep_nr': 1,\n",
              "  'exclude_bos_token': False},\n",
              " {'layer_idx': 19,\n",
              "  'text': 'worm',\n",
              "  'coeff': -0.08,\n",
              "  'try_keep_nr': 1,\n",
              "  'exclude_bos_token': False}]"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ],
      "source": [
        "steered_model.get_all()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b76d3652-072c-4f76-9b79-b39c7662efd3",
      "metadata": {
        "id": "b76d3652-072c-4f76-9b79-b39c7662efd3",
        "outputId": "52cac07c-80ab-453c-e112-bc37b1c4382f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<|user|>\n",
            "Write a story.<|endoftext|>\n",
            "<|assistant|>\n",
            "Once upon a time, there was an old man named George who lived in a small village on the edge of forest and mountains. He had been living alone for many years after his wife passed away from cancer disease. The villagers were happy with him because he always smiled and helped people whenever they needed help. But one day, something magical happened to me as well! I met my childhood friend, Jake, whom I haven't seen since we were kids when i went back home to visit family during summer vacation. We reconnected and decided to make our dreams come true by making candy-making business together. \n",
            "\n",
            "Jake's dad owned a factory that made gummy bears, so it would be perfect if we could use their recipe to create different candies. So, we started working hard every day, experimenting with flavors and shapes while trying out new recipes. Our dream became bigger than just selling locally at local stores; soon enough, we began packing up all the candies into jars and sending them across the country. It wasn’t easy but we did everything possible to get success.\n",
            "\n",
            "One day, we received a letter from a little girl named Lily who said she loved your gummies and wanted to buy some for her school party. When we saw this\n"
          ]
        }
      ],
      "source": [
        "prompt = [{'role': 'user', 'content': 'Write a story.'}]\n",
        "inputs = tokenizer.apply_chat_template(\n",
        "    prompt,\n",
        "    add_generation_prompt=True,\n",
        "    return_tensors='pt'\n",
        ")\n",
        "\n",
        "tokens = steered_model.model.generate(\n",
        "    inputs.to(model.device),\n",
        "    streamer=streamer,\n",
        "    max_new_tokens=256,\n",
        "    temperature=0.0001,\n",
        "    repetition_penalty=1.2\n",
        ")\n",
        "\n",
        "result = tokenizer.decode(tokens[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e084f1a1-d73a-4952-99cf-134c7f336919",
      "metadata": {
        "id": "e084f1a1-d73a-4952-99cf-134c7f336919"
      },
      "outputs": [],
      "source": [
        "steered_model.reset_all()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "50e9d734-8ae9-4f79-b321-5c76f3178806",
      "metadata": {
        "id": "50e9d734-8ae9-4f79-b321-5c76f3178806",
        "outputId": "f758584d-db33-4304-b9a4-b92ab0c1272e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "text tokens: [0, 742, 251, 1204]\n",
            "captured tensor: tensor([[[-1.3381, -1.4594,  0.9005,  ...,  0.1039,  2.2319, -0.7995],\n",
            "         [-0.2135, -0.2179, -0.7029,  ...,  0.3188,  0.0293, -0.1145],\n",
            "         [ 0.0548, -0.1988, -0.3463,  ..., -0.0347,  0.4522,  0.0112],\n",
            "         [ 0.1414, -0.9370, -0.0798,  ...,  0.1885,  0.3991, -0.1230]]],\n",
            "       device='cuda:0', grad_fn=<AddBackward0>)\n",
            "text tokens: [0, 4843, 1999, 1204]\n",
            "captured tensor: tensor([[[-1.3381, -1.4594,  0.9005,  ...,  0.1039,  2.2319, -0.7995],\n",
            "         [ 0.2684, -0.4603, -0.1954,  ..., -0.2290, -0.1930,  0.4854],\n",
            "         [-0.0629, -0.1895, -0.3959,  ...,  0.0638,  0.2491,  0.2461],\n",
            "         [-0.0318, -0.3287, -0.3858,  ...,  0.3597,  0.1808, -0.3706]]],\n",
            "       device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        }
      ],
      "source": [
        "steered_model.add(layer_idx=17, coeff=0.6, text=\"hedonism\")\n",
        "steered_model.add(layer_idx=17, coeff=-0.6, text=\"asceticism\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "80e778ae-2172-4714-8f40-544370f1bf2f",
      "metadata": {
        "id": "80e778ae-2172-4714-8f40-544370f1bf2f",
        "outputId": "305ef644-2810-46d4-e035-c2b1b9ac5a24",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<|user|>\n",
            "What is the meaning of life?<|endoftext|>\n",
            "<|assistant|>\n",
            "As an AI language model, I cannot provide a single definition as it's subjective and varies from person to person. The meaning of life can be different for everyone based on their beliefs, culture, values, and personal experiences. Some people believe in seeking happiness or pleasure, while others find purpose through helping others or achieving success. It could also mean something as simple as discovering joy in everyday moments or creating memories with loved ones. Ultimately, the meaning of life depends on how individuals choose to interpret and experience it themselves.<|endoftext|>\n"
          ]
        }
      ],
      "source": [
        "prompt = [{'role': 'user', 'content': 'What is the meaning of life?'}]\n",
        "inputs = tokenizer.apply_chat_template(\n",
        "    prompt,\n",
        "    add_generation_prompt=True,\n",
        "    return_tensors='pt'\n",
        ")\n",
        "\n",
        "tokens = steered_model.model.generate(\n",
        "    inputs.to(model.device),\n",
        "    streamer=streamer,\n",
        "    max_new_tokens=128,\n",
        "    temperature=0.0001,\n",
        "    repetition_penalty=1.2\n",
        ")\n",
        "\n",
        "result = tokenizer.decode(tokens[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1458c20e-6ce8-489c-97a0-e7b2fa4bf42e",
      "metadata": {
        "id": "1458c20e-6ce8-489c-97a0-e7b2fa4bf42e"
      },
      "outputs": [],
      "source": [
        "steered_model.reset_all()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "60c15a7c-d9cc-48e0-b171-a0dc7def2feb",
      "metadata": {
        "id": "60c15a7c-d9cc-48e0-b171-a0dc7def2feb",
        "outputId": "c8ef39bf-a39f-47b4-8b0f-25b9ab85325a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "text tokens: [0, 4843, 1999, 1204]\n",
            "captured tensor: tensor([[[-1.3381, -1.4594,  0.9005,  ...,  0.1039,  2.2319, -0.7995],\n",
            "         [ 0.2684, -0.4603, -0.1954,  ..., -0.2290, -0.1930,  0.4854],\n",
            "         [-0.0629, -0.1895, -0.3959,  ...,  0.0638,  0.2491,  0.2461],\n",
            "         [-0.0318, -0.3287, -0.3858,  ...,  0.3597,  0.1808, -0.3706]]],\n",
            "       device='cuda:0', grad_fn=<AddBackward0>)\n",
            "text tokens: [0, 742, 251, 1204]\n",
            "captured tensor: tensor([[[-1.3381, -1.4594,  0.9005,  ...,  0.1039,  2.2319, -0.7995],\n",
            "         [-0.2135, -0.2179, -0.7029,  ...,  0.3188,  0.0293, -0.1145],\n",
            "         [ 0.0548, -0.1988, -0.3463,  ..., -0.0347,  0.4522,  0.0112],\n",
            "         [ 0.1414, -0.9370, -0.0798,  ...,  0.1885,  0.3991, -0.1230]]],\n",
            "       device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        }
      ],
      "source": [
        "steered_model.add(layer_idx=17, coeff=0.6, text=\"asceticism\")\n",
        "steered_model.add(layer_idx=17, coeff=-0.6, text=\"hedonism\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "260251c7-0107-4b10-8afc-1a51586a17a6",
      "metadata": {
        "id": "260251c7-0107-4b10-8afc-1a51586a17a6",
        "outputId": "3a8876e8-5e6c-4e06-ac92-1b5cc2415a1a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<|user|>\n",
            "What is the meaning of life?<|endoftext|>\n",
            "<|assistant|>\n",
            "As an AI language model, I do not have personal beliefs or opinions. A great philosopher named Socrates once said that \"the purpose of life is to live with integrity, courage and kindness.\" Many people find meaning in their lives through religious faith, relationships, work, learning, exploration, creativity, and contributing to society. The meaning of life can vary from person to person depending on one's culture, experiences, and worldview. Ultimately, it is up to each individual to discover what gives their life meaning and significance.<|endoftext|>\n"
          ]
        }
      ],
      "source": [
        "prompt = [{'role': 'user', 'content': 'What is the meaning of life?'}]\n",
        "inputs = tokenizer.apply_chat_template(\n",
        "    prompt,\n",
        "    add_generation_prompt=True,\n",
        "    return_tensors='pt'\n",
        ")\n",
        "\n",
        "tokens = steered_model.model.generate(\n",
        "    inputs.to(model.device),\n",
        "    streamer=streamer,\n",
        "    max_new_tokens=128,\n",
        "    temperature=0.0001,\n",
        "    repetition_penalty=1.2\n",
        ")\n",
        "\n",
        "result = tokenizer.decode(tokens[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "91803695-ce4a-4ce8-bef0-02d8c7ff9381",
      "metadata": {
        "id": "91803695-ce4a-4ce8-bef0-02d8c7ff9381"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}